import streamlit as st
import tiktoken
import json
import os
import requests
from loguru import logger
from concurrent import futures
import pickle
import base64
from github import Github
from datetime import datetime

from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.callbacks import get_openai_callback
from langchain.document_loaders import TextLoader
from langchain.docstore.document import Document

def validate_api_key(api_key):
    """OpenAI API í‚¤ í˜•ì‹ ê²€ì¦"""
    return api_key and len(api_key) > 20

def save_vectorstore_to_github(vectorstore, github_token, repo_name, branch="main"):
    """ë²¡í„° ì €ì¥ì†Œë¥¼ GitHubì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ë²¡í„° ì €ì¥ì†Œë¥¼ ë°”ì´íŠ¸ë¡œ ì§ë ¬í™”
        serialized_vectorstore = pickle.dumps(vectorstore)
        
        # base64ë¡œ ì¸ì½”ë”©
        encoded_content = base64.b64encode(serialized_vectorstore).decode()
        
        # GitHub API ì—°ê²°
        g = Github(github_token)
        repo = g.get_user().get_repo(repo_name)
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_path = f"vectorstore/vectorstore_{timestamp}.pkl"
        
        try:
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            contents = repo.get_contents(file_path, ref=branch)
            # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸
            repo.update_file(
                file_path,
                f"Update vector store {timestamp}",
                encoded_content,
                contents.sha,
                branch=branch
            )
        except:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            repo.create_file(
                file_path,
                f"Create vector store {timestamp}",
                encoded_content,
                branch=branch
            )
        
        # ì„±ê³µ ë©”ì‹œì§€ ë°˜í™˜
        return True, file_path
        
    except Exception as e:
        logger.error(f"GitHub ì €ì¥ ì˜¤ë¥˜: {e}")
        return False, str(e)

def load_vectorstore_from_github(github_token, repo_name, file_path, branch="main"):
    """GitHubì—ì„œ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        # GitHub API ì—°ê²°
        g = Github(github_token)
        repo = g.get_user().get_repo(repo_name)
        
        # íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        content = repo.get_contents(file_path, ref=branch)
        
        # base64 ë””ì½”ë”©
        decoded_content = base64.b64decode(content.content)
        
        # ë²¡í„° ì €ì¥ì†Œë¡œ ì—­ì§ë ¬í™”
        vectorstore = pickle.loads(decoded_content)
        
        return True, vectorstore
        
    except Exception as e:
        logger.error(f"GitHub ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False, str(e)

def process_json(file_path):
    """JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # JSON ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        text_content = json.dumps(data, ensure_ascii=False, indent=2)
        
        # Document ê°ì²´ ìƒì„±
        return [Document(
            page_content=text_content,
            metadata={"source": file_path}
        )]
    except Exception as e:
        logger.error(f"JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def tiktoken_len(text):
    """í…ìŠ¤íŠ¸ì˜ í† í° ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜"""
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def get_text(docs):
    """ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    doc_list = []
    
    for doc in docs:
        try:
            # ë¬¸ì„œ ì €ì¥
            file_name = doc.name
            with open(file_name, "wb") as file:
                file.write(doc.getvalue())
                logger.info(f"ì—…ë¡œë“œëœ íŒŒì¼: {file_name}")

            # íŒŒì¼ ìœ í˜•ì— ë”°ë¼ ë¡œë” ì„ íƒ
            if '.pdf' in doc.name.lower():
                loader = PyPDFLoader(file_name)
                documents = loader.load_and_split()
            elif '.docx' in doc.name.lower():
                loader = Docx2txtLoader(file_name)
                documents = loader.load_and_split()
            elif '.pptx' in doc.name.lower():
                loader = UnstructuredPowerPointLoader(file_name)
                documents = loader.load_and_split()
            elif '.json' in doc.name.lower():
                documents = process_json(file_name)
            else:
                continue

            doc_list.extend(documents)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_name}, ì˜¤ë¥˜: {e}")
            continue
            
    return doc_list

def get_text_chunks(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,     # ì²­í¬ í¬ê¸°
        chunk_overlap=100,  # ì²­í¬ ê°„ ì¤‘ë³µ í…ìŠ¤íŠ¸ ê¸¸ì´
        length_function=tiktoken_len  # í† í° ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜
    )
    chunks = text_splitter.split_documents(text)
    return chunks

def get_conversation_chain(vectorstore, openai_api_key):
    """ëŒ€í™” ì²´ì¸ ìƒì„± í•¨ìˆ˜"""
    # OpenAI ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-4', temperature=0)
    
    # ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±
    conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, 
            chain_type="stuff", 
            retriever=vectorstore.as_retriever(search_type='mmr', verbose=True), 
            memory=ConversationBufferMemory(
                memory_key='chat_history',
                return_messages=True,
                output_key='answer'
            ),
            get_chat_history=lambda h: h,
            return_source_documents=True,
            verbose=True
        )

    return conversation_chain

def main():
    # í˜ì´ì§€ ì„¤ì •
    st.set_page_config(
        page_title="ìš”ë¦¬ ë„ìš°ë¯¸",
        page_icon="ğŸ³"
    )

    st.title("ìš”ë¦¬ ë„ìš°ë¯¸ ğŸ³")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None
    if "processComplete" not in st.session_state:
        st.session_state.processComplete = False
    if 'messages' not in st.session_state:
        st.session_state['messages'] = [
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìš”ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì–´ë–¤ ìš”ë¦¬ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"}
        ]
    if 'vectorstore_path' not in st.session_state:
        st.session_state.vectorstore_path = None

    # ì‚¬ì´ë“œë°” ìƒì„±
    with st.sidebar:
        st.header("ì„¤ì •")
        
        # GitHub ì„¤ì •
        github_token = st.text_input("GitHub Token", type="password")
        repo_name = st.text_input("Repository Name", "AI_8_CH-3_LLM-RAG_AI_Utilizatioon_App")
        
        # ë¬¸ì„œ ì—…ë¡œë“œ ì„¹ì…˜
        st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "ìš”ë¦¬ ê´€ë ¨ ë¬¸ì„œ ì—…ë¡œë“œ",
            type=["pdf", "docx", "pptx", "json"],
            accept_multiple_files=True
        )
        
        # OpenAI API í‚¤
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        if not openai_api_key:
            st.info("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸ”‘")
        
        # ë¬¸ì„œ ì²˜ë¦¬ ë²„íŠ¼
        col1, col2 = st.columns(2)
        with col1:
            process_button = st.button("ë¬¸ì„œ ì²˜ë¦¬")
        with col2:
            load_button = st.button("ë²¡í„° ë¶ˆëŸ¬ì˜¤ê¸°")

    # ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
    if load_button:
        if not github_token:
            st.error("GitHub í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()
            
        if not validate_api_key(openai_api_key):
            st.error("ìœ íš¨í•œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        try:
            with st.spinner("ë²¡í„° ì €ì¥ì†Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                # ê°€ì¥ ìµœê·¼ ë²¡í„° ì €ì¥ì†Œ íŒŒì¼ ì°¾ê¸°
                g = Github(github_token)
                repo = g.get_user().get_repo(repo_name)
                contents = repo.get_contents("vectorstore", ref="main")
                latest_file = max(contents, key=lambda c: c.path)
                
                # ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
                success, result = load_vectorstore_from_github(
                    github_token,
                    repo_name,
                    latest_file.path
                )
                
                if success:
                    vectorstore = result
                    st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)
                    st.session_state.processComplete = True
                    st.session_state.vectorstore_path = latest_file.path
                    st.success(f"ë²¡í„° ì €ì¥ì†Œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤! (íŒŒì¼: {latest_file.path})")
                else:
                    st.error(f"ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {result}")
                    
        except Exception as e:
            st.error(f"ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")

    # ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§
    if process_button:
        if not validate_api_key(openai_api_key):
            st.error("ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        if not uploaded_files:
            st.warning("ì²˜ë¦¬í•  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            st.stop()
            
        if not github_token:
            st.error("GitHub í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        try:
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                # ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                docs = get_text(uploaded_files)
                
                # í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±
                chunks = get_text_chunks(docs)
                
                # ì„ë² ë”© ìƒì„±
                embeddings = HuggingFaceEmbeddings(
                    model_name="jhgan/ko-sroberta-multitask",
                    model_kwargs={'device': 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                
                # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)
                
                # GitHubì— ë²¡í„° ì €ì¥ì†Œ ì €ì¥
                success, result = save_vectorstore_to_github(
                    vectorstore,
                    github_token,
                    repo_name
                )
                
                if success:
                    st.session_state.vectorstore_path = result
                    st.success(f"ë²¡í„° ì €ì¥ì†Œë¥¼ GitHubì— ì €ì¥í–ˆìŠµë‹ˆë‹¤! (ê²½ë¡œ: {result})")
                else:
                    st.warning(f"ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {result}")
                
                # ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™”
                st.session_state.conversation = get_conversation_chain(vectorstore, openai_api_key)
                st.session_state.processComplete = True
                st.success("ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")

        except Exception as e:
            st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

   # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": query})
    
    with st.chat_message("user"):
        st.write(query)

    if not st.session_state.conversation:
        st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”.")
        st.session_state.messages.append({
            "role": "assistant", 
            "content": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¨¼ì € ìš”ë¦¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”."
        })
        st.rerun()

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            try:
                result = st.session_state.conversation({"question": query})
                response = result['answer']
                source_documents = result.get('source_documents', [])

                st.write(response)

                if source_documents:
                    with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                        for i, doc in enumerate(source_documents[:3], 1):
                            st.markdown(f"**ì°¸ê³  {i}:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}")
                            st.markdown(f"```\n{doc.page_content[:200]}...\n```")

                st.session_state.messages.append({"role": "assistant", "content": response})

            except Exception as e:
                error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_message)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_message
                })
                logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
