import streamlit as st
import logging
import pickle
import json
import os
from datetime import datetime
from gtts import gTTS
import base64
import tempfile
import requests

from langchain.chat_models import ChatOpenAI
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "ì•ˆë…•í•˜ì„¸ìš”! ìš”ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì–´ë–¤ ìš”ë¦¬ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                "audio": None
            }
        ]
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "custom_prompt" not in st.session_state:
        st.session_state.custom_prompt = """
ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
ë‹µë³€: ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤.
"""
    if "voice_enabled" not in st.session_state:
        st.session_state.voice_enabled = True

def autoplay_audio(audio_content, autoplay=True):
    """ìŒì„± ìë™ ì¬ìƒì„ ìœ„í•œ HTML ì»´í¬ë„ŒíŠ¸ ìƒì„±"""
    b64 = base64.b64encode(audio_content).decode()
    autoplay_attr = 'autoplay' if autoplay else ''
    md = f"""
        <audio controls {autoplay_attr}>
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
        </audio>
        """
    st.markdown(md, unsafe_allow_html=True)

def text_to_speech(text, lang='ko'):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
            tts = gTTS(text=text, lang=lang)
            tts.save(fp.name)
        with open(fp.name, 'rb') as audio_file:
            audio_bytes = audio_file.read()
        os.remove(fp.name)
        return audio_bytes
    except Exception as e:
        logger.error(f"ìŒì„± ë³€í™˜ ì˜¤ë¥˜: {e}")
        return None

def load_vector_database():
    """GitHub ì €ì¥ì†Œì—ì„œ JSON íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œ"""
    repo_path = "Dawol2205/chatbot_test"
    folder_path = "food_DB"
    api_url = f"https://api.github.com/repos/{repo_path}/contents/{folder_path}"

    response = requests.get(api_url)
    files = response.json()

    documents = []
    for file in files:
        if file['name'].endswith('.json'):
            content_response = requests.get(file['download_url'])
            content = content_response.json()
            metadata = {'source': file['name']}
            doc = Document(page_content=json.dumps(content, ensure_ascii=False), metadata=metadata)
            documents.append(doc)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')

    if os.path.exists("index.pkl") and os.path.exists("index.faiss"):
        with open("index.pkl", "rb") as f:
            vector_store = pickle.load(f)
    else:
        vector_store = FAISS.from_documents(texts, embeddings)
        with open("index.pkl", "wb") as f:
            pickle.dump(vector_store, f)
        vector_store.save_local("index.faiss")

    return vector_store

def validate_api_key(api_key):
    """OpenAI API í‚¤ í˜•ì‹ ê²€ì¦"""
    return api_key and len(api_key) > 20

def get_conversation_chain(vectorstore, openai_api_key, custom_prompt):
    """ëŒ€í™” ì²´ì¸ ìƒì„±"""
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-4', temperature=0)
    
    PROMPT = PromptTemplate(
        template=custom_prompt,
        input_variables=["context", "question"]
    )
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_type='mmr', verbose=True),
        memory=ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True,
            output_key='answer'
        ),
        combine_docs_chain_kwargs={"prompt": PROMPT},
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True
    )

    return conversation_chain

def main():
    try:
        # í˜ì´ì§€ ì„¤ì •
        st.set_page_config(
            page_title="ìš”ë¦¬ ë„ìš°ë¯¸",
            page_icon="ğŸ³",
            layout="wide",
            initial_sidebar_state="expanded"
        )

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        initialize_session_state()

        st.title("ìš”ë¦¬ ë„ìš°ë¯¸ ğŸ³")

        # ì‚¬ì´ë“œë°” ì„¤ì •
        with st.sidebar:
            st.header("ì„¤ì •")
            
            # ìŒì„± ì¶œë ¥ í† ê¸€
            st.session_state.voice_enabled = st.toggle("ìŒì„± ì¶œë ¥ í™œì„±í™”", value=st.session_state.voice_enabled)
            
            # API í‚¤ ì…ë ¥
            openai_api_key = st.text_input("OpenAI API Key", type="password")
            if not openai_api_key:
                st.info("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸ”‘")

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
            st.header("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿")
            custom_prompt = st.text_area("RAG í”„ë¡¬í”„íŠ¸", value=st.session_state.custom_prompt)
            if custom_prompt != st.session_state.custom_prompt:
                st.session_state.custom_prompt = custom_prompt

            # GitHub íŒŒì¼ ì²˜ë¦¬ ì„¹ì…˜
            st.header("ë°ì´í„° ë¡œë“œ")
            if st.button("ìš”ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
                if not validate_api_key(openai_api_key):
                    st.error("ìœ íš¨í•œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    st.stop()

                try:
                    with st.spinner("ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                        vector_store = load_vector_database()
                        
                        # ì„¸ì…˜ì— ì €ì¥
                        st.session_state.vectorstore = vector_store
                        st.session_state.conversation = get_conversation_chain(
                            vector_store, 
                            openai_api_key,
                            st.session_state.custom_prompt
                        )
                        st.success("ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.write(message["content"])
                    # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ì— ëŒ€í•´ ìŒì„± ì»¨íŠ¸ë¡¤ ì¶”ê°€
                    if message["role"] == "assistant" and st.session_state.voice_enabled:
                        if message.get("audio") is None and message["content"]:
                            # ìŒì„±ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ìƒì„±
                            audio_bytes = text_to_speech(message["content"])
                            if audio_bytes:
                                message["audio"] = audio_bytes

                        if message.get("audio"):
                            # ìŒì„± ì»¨íŠ¸ë¡¤ í‘œì‹œ
                            cols = st.columns([1, 4])
                            with cols[0]:
                                if st.button("ğŸ”Š ì¬ìƒ", key=f"play_message_{message['content']}"):
                                    autoplay_audio(message["audio"])
                            with cols[1]:
                                # ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ í‘œì‹œ (ì»¨íŠ¸ë¡¤ í¬í•¨)
                                autoplay_audio(message["audio"], autoplay=False)

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.session_state.messages.append({"role": "user", "content": query, "audio": None})
            
            with st.chat_message("user"):
                st.write(query)

            if not st.session_state.conversation:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¨¼ì € ìš”ë¦¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì£¼ì„¸ìš”."
                st.warning(response)
                
                if st.session_state.voice_enabled:
                    audio_bytes = text_to_speech(response)
                else:
                    audio_bytes = None
                    
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "audio": audio_bytes
                })
                
                if audio_bytes:
                    autoplay_audio(audio_bytes)
                
                st.stop()

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    try:
                        result = st.session_state.conversation({"question": query})
                        response = result['answer']
                        source_documents = result.get('source_documents', [])

                        st.write(response)

                        # ìŒì„± ì¶œë ¥ ì²˜ë¦¬
                        if st.session_state.voice_enabled:
                            audio_bytes = text_to_speech(response)
                            if audio_bytes:
                                message = st.session_state.messages[-1]
                                message["audio"] = audio_bytes
                                autoplay_audio(audio_bytes, autoplay=False)
                        else:
                            audio_bytes = None

                        if source_documents:
                            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                                for i, doc in enumerate(source_documents[:3], 1):
                                    st.markdown(f"**ì°¸ê³  {i}:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}")
                                    st.markdown(f"```\n{doc.page_content[:200]}...\n```")

                        st.session_state.messages.append({
                            "role": "assistant", 
                            "content": response,
                            "audio": audio_bytes
                        })

                    except Exception as e:
                        error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                        st.error(error_message)
                        
                        if st.session_state.voice_enabled:
                            audio_bytes = text_to_speech(error_message)
                        else:
                            audio_bytes = None
                            
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": error_message,
                            "audio": audio_bytes
                        })
                        
                        if audio_bytes:
                            autoplay_audio(audio_bytes)
                        
                        logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")

    except Exception as e:
        logger.error(f"ì•± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ì•± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ì„ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == '__main__':
    main()
